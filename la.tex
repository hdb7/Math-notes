%%---------------------------------------------------------%%
%%				NOTES TEMPLATE
%% ---------------------------------------------------------%%

\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,scrextend}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{xcolor}
\usepackage{circledsteps}
%% math macros
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\W}{\mathbb{W}}
%\renewcommand{\qed}{\hfill$\blacksquare$}

\let\newproof\proof
\renewenvironment{proof}{\begin{addmargin}[1em]{0em}\begin{newproof}}{\end{newproof}\end{addmargin}\qed}
% \newcommand{\expl}[1]{\text{\hfill[#1]}$}
 \newtheorem{thm}{Theorem}
 \newtheorem*{defn}{Definition}
 \newtheorem{conv}{Convention}
 \newtheorem{rem}{Remark}
 \newtheorem{lem}{Lemma}
 \newtheorem{cor}{Corollary}
 \newtheorem{ex}{Example}
 

\begin{document}
 

\lhead{Linear Algebra}
\chead{Hamjak}
\rhead{\today}
 
% \maketitle

\begin{defn}\normalfont
	Set $F$ with $2$ operation: $\cdot, +$ such that:
	\begin{itemize}
		\item $(F, +)$ is a commutative group.
		\item $(F, \cdot)$ is a commutative group
		\item $a(b+c) = ab+ac$
		\item $(b+c)a = ba+ca$
	\end{itemize}
\end{defn}

\textbf{Note: } A field is basically a set that consist of all the $4$ operations: $+,/,-,\times$

\begin{defn}\normalfont
	A vector space $V$ over a field $\R$ is a set on which two operations: vector addition ($+$) and scalar multiplication($\cdot$) are defined such that -
	\begin{itemize}
		\item $(V,+)$ forms commutative group
		\item The operation ($\times$) is define between scalars and vectors such that: 
		$\forall  a \in \R \;\text{and}\; v \in V \implies av \in V$ 
		\item $\forall a \in \R \;\text{and}\; v,w \in V \implies a(v+w)=av+aw$
		\item $\forall a,b \in \R \;\text{and}\; v \in V,\; (a+b)v = av+bv$
		\item $\forall a,b \in \R \;\text{and}\; v \in V,\; (ab)v = a(bv)$
		\item Unitary Law: $\exists 1\in \R$ s.t $1 \cdot v = v\; \forall v \in V$
	\end{itemize}
$(V, +, \cdot)$ is a vector space
\end{defn}

\begin{ex}\normalfont
	$V = P(\R)$ where, $P$ is set of all polynomials in variable $x$ and coefficient from $\R$
\end{ex}

\begin{ex}\normalfont
	$V =$ set of all polynomial of degree $n$ does not form a vector space over $\R$
\end{ex}

\begin{defn}\normalfont
	The vector in \textbf{Euclidean Space} consist of $n-$tuple of $\R$, $(x_1,x_2,\dots x_n)$, Euclidean Space is used mathemarically represent physical space with notions such as distance, length and angles.
\end{defn}

\begin{defn} \normalfont
	If $\R^n$ is a vector space, then $S \subseteq \R^n$ is said to be a subspace of $\R^n (\R)$ if - 
	\begin{itemize}
		\item $0 \in S$
		\item $S$ is closed under vector addition: $x,y \in S \implies x+y \in S$
		\item $S$ is closed under scalar multiplication: $x \in S, \alpha \in \R \implies \alpha x \in S$
	\end{itemize}
\end{defn}

\begin{defn}[Aliter]\normalfont
	If $\R^n(R)$ a vector space then $S \subseteq \R^n$ is said to be subspace if $S(\R)$
\end{defn}

\begin{thm}\normalfont
	Let $W \subseteq \R^n$ then $W$ is a subspace of $\R^n$ iff-
	\begin{itemize}
		\item $W$ is non-empty
		\item For any $a,b \in \R $ and any $\vec{u}, \vec{v} \in W,\; a\vec{u}+b\vec{v} \in W$
	\end{itemize}
\end{thm}


\begin{defn}\normalfont
	A matrix represent a collection of numbers in rows and cols.
\end{defn}
$$ 
A = \begin{pmatrix}
	a_{11} & a_{12} & a_{13} \\
	a_{21} & a_{22} & a_{23} \\
	a_{31} & a_{32} & a_{33} 
\end{pmatrix} 
$$
\textbf{Order of matrix} $\#rows \times \#cols$
$$ 
A_{3\times 3} = \begin{pmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33} 
\end{pmatrix}
$$
\textbf{\textit{Note:}} by matrix we consider the square matrix \\
\textbf{Operations on Matrices}\\
\textbf{Addition/ Subtraction:} $A=(a_{i,j})_{m\times n}$ and $B=(b_{i,j})_{m\times n} \implies A\pm B = (a_{i,j} \pm b_{i,j})_{m\times n} $
\textbf{Properties of Addition:}
\begin{itemize}
	\item Commutative : $A+B=B+A$
	\item Associative : $(A+B)+C = A+(B+C)$
\end{itemize}
\textbf{Multiplication: } iff $\#cols(A)=\#rows(B) \implies AB_{rows(A)\times cols(B)}$\\
In general, $A=[a_{i,j}]_{m\times n}$ and  $B=[b_{i,j}]_{n\times p}$ then $AB=[c_{i,k}]_{m\times p}$ where, $c_{i,k} = \sum_{j=1}^{n} a_{ij}b_{jk}$ 
\textbf{Properties of Multiplication:}
\begin{itemize}
	\item Not Commutative : $AB\not=BA$
	\item Associative : $(AB)C = A(BC)$
\end{itemize}
\textbf{Scalar Multiplication} multiplying each elements with a real number. Let $A = [a_{i,j}]$ and let $k \in \R$ then $kA = [ka_{i,j}]$. \\
\textbf{Transpose of Matrix: }interchange of rows and cols, denoted as $A^T$ \\
\textbf{Types of Matrices}\\
\textbf{Square Matrix: } matrix of same order i.e $A_{n \times n}$\\
\textbf{Upper triangular Matrix: } $A_{n \times n}$ s.t all the elements below the main diagonal are $0$ i.e $A = [a_{ij}] \impliedby a_{ij}=0\: \forall i>j$\\
\textbf{Lower triangular Matrix: } $A_{n \times n}$ s.t all the elements above the main diagonal are $0$ i.e $A = [a_{ij}] \impliedby a_{ij}=0\: \forall i<j$\\
\textbf{Symmetric Matrix: } $A^T = A$ (above/below diagonal elements are same)\\
\textbf{Skew Symmetric Matrix (Anti-symmetric): } $A^T=-A$\\
\textbf{Diagonal Matrix: } $A = [a_{ij}]_{n \times n}$ where, $a_{ij}=0\: \forall i\not = j$\\
\textbf{Identity or Unit Matrix: } $I = [a_{ij}]_{n \times n}$ where, 
\[ 
a_{ij}= \begin{cases}
	1 & \text{if i = j}\\
	0 & \text{Otherwise}
\end{cases}
\]\\
\textbf{Orthogonal Matrix: } $A^TA = AA^T=I$\\
\textbf{Idempotent Matrix: } $A^2 = A$\\
\textbf{Involutary Matrix: } $A^2 = I$\\
\textbf{Singular Matrix: } $|A| = 0$\\
\textbf{Non-Singular Matrix: } $|A| \not= 0$\\
\\
\textbf{Minor of $a_{ij}$} denoted as $M_{ij}$ is obtained by deleting $i^{th}$ rows and $j^{th}$cols\\
\[
	\delta = \begin{vmatrix}
	a_{11} & a_{12} & a_{13} \\
	a_{21} & a_{22} & a_{23} \\
	a_{31} & a_{32} & a_{33} 
	\end{vmatrix}
	\implies
	M_{11} = \begin{vmatrix}
	 a_{22} & a_{23} \\
	 a_{32} & a_{33}
	\end{vmatrix}
\]
\textbf{Co-factor of Matrix}: $a_{ij} = (-1)^{i+j} M_{ij}$\\
\textbf{Adjoint of Matrix } matrix obtained by taking the transpose of cofactor matrix of a given matrix.\\
\textbf{Inverse of Matrix}: $ A^{-1} = \frac{adj(A)}{|A|}$ where, $|A| \not=0$
\\
\\
\begin{thm}\normalfont
	$Aadj(A) = adj(A)A = |A|.I$
\end{thm}
\begin{thm}\normalfont
	$A$ is said to be invertible iff $AA^{-1} = A^{-1}A = I$
\end{thm}
\begin{thm}\normalfont
	Invertible matrix has an unique inverse
\end{thm}
\begin{thm}\normalfont
	If $A$ and $B$ are invertible then  $AB$ also invertible s.t $(AB)^{-1}= B^{-1}A^{-1}$
\end{thm}
\begin{thm}\normalfont
 If $A$ is invertible then $A^T$ is also invertible.
\end{thm}
\begin{thm}\normalfont
If $A$ is invertible symmetric then $A^{-1} $ also symmetric
\end{thm}
\begin{thm}\normalfont
 If $A$ and $B$ are nonsingular then $adj(AB) = adj(B)adj(A)$
\end{thm}
\begin{thm}\normalfont
 If $|A |\not= 0 \implies |adj(A) |= |A|^{n-1}$
\end{thm}
\begin{thm}\normalfont
	If $|A|\not= 0 \implies adj(adj(A)) = |A|^{n-2}.A$
\end{thm}

\begin{defn}\normalfont
	Determinants are scalar quantities that can be calculated from a square matrix. Denoted as $det(A)$ or $|A|$.
\end{defn}
\textbf{Expansion of determinant:} $|A| = a_{ij} + Cofactor \:of \:a_{ij}$\\
\textbf{Properties of Determinant}
\begin{itemize}
	\item Determinant evaluated across any rows/col are same.
	\item If all elements of row or col are $0$ then the $det(A)=0$
	\item $|I_n|=I$
	\item $|A^T|=|A|$
	\item $|AB|=|A||B|=|B||A|$
	\item $|A^n|=|A|^n$
	\item The interchange of any two rows or cols changes the sign of a determinant without altering its absolute value.
	\[
	  \begin{vmatrix}
	  	a_{11} & a_{12} & a_{13} \\
	  	a_{21} & a_{22} & a_{23} \\
	  	a_{31} & a_{32} & a_{33} 
	  \end{vmatrix}
	  = -
	  \begin{vmatrix}
	  a_{21} & a_{22} & a_{23} \\
	  a_{11} & a_{12} & a_{13} \\
	  a_{31} & a_{32} & a_{33} 
	  \end{vmatrix}	  
	\]
	\item If two rows or cols in a det. are same the value of det is $0$
	\item If the elements of row or col of a det is multiplied by scalar, then the value of a new det is equal to some scalar times the value of original det.
	\[
		\begin{vmatrix}
		ka_{11} & ka_{12} & ka_{13} \\
		a_{21} & a_{22} & a_{23} \\
		a_{31} & a_{32} & a_{33} 
		\end{vmatrix}
		=k
			\begin{vmatrix}
		a_{11} & a_{12} & a_{13} \\
		a_{21} & a_{22} & a_{23} \\
		a_{31} & a_{32} & a_{33} 
		\end{vmatrix}
	\]
	\item In a det each elements in any row or col consist of the sum of two terms, then the det can be expressed as sum of two det of same order.
	\[
		\begin{vmatrix}
		a_{11}+x_1 & a_{12} & a_{13} \\
		a_{21}+x_2 & a_{22} & a_{23} \\
		a_{31}+x_3 & a_{32} & a_{33} 
		\end{vmatrix}
		=
			\begin{vmatrix}
		a_{11} & a_{12} & a_{13} \\
		a_{21} & a_{22} & a_{23} \\
		a_{31} & a_{32} & a_{33} 
		\end{vmatrix}
		+
		\begin{vmatrix}
		x_1 & a_{12} & a_{13} \\
		x_2 & a_{22} & a_{23} \\
		x_3 & a_{32} & a_{33} 
		\end{vmatrix}	
	\]
	\item If $ |A| = \begin{vmatrix}
	a_{1} & a_{2} & a_{3} \\
	b_{1} & b_{2} & b_{3} \\
	c_{1} & c_{2} & c_{3} 
	\end{vmatrix}  $ $ |B| = \begin{vmatrix}
	b_1+ca_{1} & b_2+ca_{2} & b_3+ca_{3} \\
	b_{1} & b_{2} & b_{3} \\
	c_{1} & c_{2} & c_{3} 
	\end{vmatrix}  $ then $|A| = |B|$
	\item Det. of a diagonal matrix, triangular(Upper or lower) matrix is product of element of principle diagonal.
	\item $|A^{-1}| = \frac{1}{|A|}$
	\item If $A_{n\times n}$ then $|kA| = k^n|A|$
\end{itemize}

\subsection*{Elementary Transformation}
\begin{itemize}
	\item Interchange of any rows or cols: $R_{i} \leftrightarrow R_j$ or $C_i \leftrightarrow C_j$
	\item Multiplication of $i^{th}$ row or col by $k\not=0$: $R_{i} \leftrightarrow kR_j$or $C_i \leftrightarrow kC_j$
	\item Addition of $k$ times the $j^{th}$ row or col to the $i^{th}$ row or col : $R_{i} \leftrightarrow R_i+ kR_j$or $C_i \leftrightarrow C_i + kC_j$
\end{itemize}

\textbf{Fact: }\textit{Trace of matrix is the sum of diagonal element} i.e for $A_{n\times n} \: \sum_{i=1}^{n} diag[a_i]$

\subsection*{Echelon Form of a Matrix}
\begin{itemize}
	\item Any rows of all zeroes are below any other non-zero rows (\textit{not always the case})
	\item Each leading entry of a row is in column to the right of leading entry of the row above it.
	\item All entries in a col below a leading entry are zeroes.
\end{itemize}
\[
	A = \begin{pmatrix}
	\Circled[outer color=cyan]{3} & 2 & 0 & 7 & 9\\
	0 & \Circled[outer color=cyan]4 & 5 & 10 & 0\\
	0 & 0 & 0 & \Circled[outer color=cyan]{-4} & 1\\
	0 & 0 & 0 & 0 &\Circled[outer color=cyan] 6\\
	0 & 0 & 0 & 0 & 0
	\end{pmatrix}
\]
Here, $	\Circled[outer color=cyan]{a_i}$ is the leading entry

\subsection*{Reduced Echelon Form}
\begin{itemize}
	\item Matrix has to be in echelon form
	\item The leading entry in each non-zero row is $1$
	\item Each leading $1$ is the only non-zero entry in its column
\end{itemize}

\[
		A = \begin{pmatrix}
	\Circled[outer color=green]{1} & 0 & 3 & 0 & 9\\
	0 & \Circled[outer color=green]1 & 4 & 0 & -6\\
	0 & 0 & 0 & \Circled[outer color=green]{1} & 1\\
	0 & 0 & 0 & 0 & 0\\
	0 & 0 & 0 & 0 & 0
	\end{pmatrix}
\]

\textbf{Fact:} Reduce Echelon Form(REF) is unique but not the Echelon Form

\subsection*{System of Linear Equation}
Consider eqn. $$a_ix+b_iy+c_iz = d_i, \: i \in \N $$
in matrix form it's represented as $AX=B$, 
\[
	\begin{bmatrix}
	a_{1} & a_{2} & a_{3} \\
	b_{1} & b_{2} & b_{3} \\
	c_{1} & c_{2} & c_{3} \\
	\end{bmatrix}
	\begin{bmatrix}
	x \\
	y \\
	z
	\end{bmatrix}
	= \begin{bmatrix}
	d_1 \\ d_2 \\ d_3
	\end{bmatrix}
\]

\textbf{Consistency of Linear Equation}
\begin{itemize}
	\item If $|A| \not= 0$ then the system is consistent and has a unique solution i.e $X = A^{-1}B$
	\item If $|A| = 0$ then the system has either no solution or have infinite \# solutions.
\end{itemize}

\textbf{Augmented Matrix Form: } $AX=B \implies [A:B]$

\[
 \begin{bmatrix}
	 a_{11} & a_{12} & \dots & a_{1m} &\bigm| & d_1 \\
	 a_{21} & a_{22} & \dots & a_{2m} &\bigm| & d_2 \\
	 \vdots & \vdots & \ddots & \vdots&\bigm| & \vdots \\
     a_{n1} & a_{n2} & \dots & a_{nm} &\bigm| & d_m \\
 \end{bmatrix}
\]

\subsection*{Homogeneous System of Equation}
Equation of the form: $AX=0$ \\
For Homog. : $[A:0]$ \\

\textbf{Consistency of Homog. System}
\begin{itemize}
	\item $X=0$ (Trivial Soln)
	\item If $\rho (A) = \# unknown\: variable$, unique soln.
	\item If $\rho (A) < \# unknown \:variable$, infinite soln. implies $det(A)=0$
\end{itemize}

\subsection*{Non-Homog. System of Equation}
Equation of the form: $AX=B, \: B \not=0$ \\
For Homog. : $[A:B]$ \\
\textbf{Consistency of Non-Homog. System}
\begin{itemize}
	\item If $\rho[A:B] \not= \rho(A)$, No solution.
	\item If $\rho [A:B] = \rho(A) = \# unknown\: variable$, unique soln.
	\item If $\rho [A:B] = \rho (A) \not = \# unknown \:variable$, infinite no. of soln.
\end{itemize}

\subsection*{Eigen Values and Eigen Vector}
\begin{defn}\normalfont
	Let $A_{n \times n}$ consider the homog. system of eqn.
	$$ AX = \lambda X \implies (A-\lambda I)X=0$$
	where, $I$ is the identity matrix, $\lambda$ is scalar. Then, $\lambda$ is called an \textit{eigen values} and non-zero vector $X$ is \textit{eigen vector}
\end{defn}
\begin{defn}\normalfont
	Polynomial obtained from $|A-\lambda I|=0$ is called the \textit{Characteristics Polynomial} and the roots of polynomial is the \textit{eigen values.}
\end{defn}

\textbf{Guassian Elimination: } convert augmented matrix to REF and solve the linear system of equation.

\textbf{Method of finding eigen values and eigen vector}
\begin{itemize}
	\item Solving the Characteristics polynomial we get the roots as eigen values.
	\item Now we find the eigen vector for its eigen values by putting $\lambda$ values in $(A-\lambda I)X=0$ and using Gaussian Elimination.
\end{itemize}



\end{document}